{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Verifiable Support Vector Machine**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machines (SVM) model is a supervised learning technique used for classification and regression. It is employed to solve binary classification problems where it identifies the hyperplane that best divides a data set into classes. This hyperplane results from maximizing the margin between the two classes. By determining this optimal hyperplane, predictions can be made for new data points and understand how the input attributes influence classification.\n",
    "\n",
    "Below, we provide a brief review of implementing an SVM model using the Gradient Descent method for the linear kernel in Python, which we will later convert to Cairo to transform it into a verifiable ZKML (support vector machine model), using Orion's library. This allows an opportunity to familiarize oneself with the main functions and operators that the framework offers for the implementation of the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **DataSet Generate**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, we generated linearly separable data using make_blobs from Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_train = np.array([\n",
    "    [5.1, 3.5, 1.4, 0.2],\n",
    "    [4.9, 3.0, 1.4, 0.2],\n",
    "    [4.7, 3.2, 1.3, 0.2],\n",
    "    [4.6, 3.1, 1.5, 0.2],\n",
    "    [5.0, 3.6, 1.4, 0.2],\n",
    "    [5.4, 3.9, 1.7, 0.4],\n",
    "    [4.6, 3.4, 1.4, 0.3],\n",
    "    [5.0, 3.4, 1.5, 0.2],\n",
    "    [4.4, 2.9, 1.4, 0.2],\n",
    "    [4.9, 3.1, 1.5, 0.1],\n",
    "    [7.0, 3.2, 4.7, 1.4],\n",
    "    [6.4, 3.2, 4.5, 1.5],\n",
    "    [6.9, 3.1, 4.9, 1.5],\n",
    "    [5.5, 2.3, 4.0, 1.3],\n",
    "    [6.5, 2.8, 4.6, 1.5],\n",
    "    [5.7, 2.8, 4.5, 1.3],\n",
    "    [6.3, 3.3, 4.7, 1.6],\n",
    "    [4.9, 2.4, 3.3, 1.0],\n",
    "    [6.6, 2.9, 4.6, 1.3],\n",
    "    [5.2, 2.7, 3.9, 1.4],\n",
    "])\n",
    "y_train = np.array([\n",
    "    -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
    "    1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "])\n",
    "\n",
    "X_test, y_test = X_train, y_train\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will visualize the training data using a scatter plot, where the points are colored based on their class labels, which in our case will be 1 and -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=10, cmap='autumn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loss function, gradient and Weight init** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by generating the key functions for SVM.\n",
    "\n",
    "Next, we'll define the loss functions and its gradient, with $\\mathbf{L2}$ regularization, both necessary to train our SVM.\n",
    "\n",
    "In the case of the loss function in SVM, the Hinge Loss ($\\max(0, 1 - y_i \\times (\\mathbf{w} \\cdot \\mathbf{x}_i))$) is used, which measures how far a sample is on the \"wrong side\" of the margin. If the sample is on the correct side of the margin, the loss is 0.\n",
    "\n",
    "$\\text{Loss Function}$  = $ \\frac{1}{N} \\sum_{i=1}^{N} \\max(0, 1 - y_i \\times (\\mathbf{w} \\cdot \\mathbf{x}_i)) + C \\times \\frac{1}{2} \\times \\mathbf{w} \\cdot \\mathbf{w}$\n",
    "\n",
    "$\\text{Gradient}$  =  $\\frac{1}{N} \\sum_{i=1}^{N} \\left( -y_i \\times \\mathbf{x}_i \\text{ (si } y_i \\times (\\mathbf{w} \\cdot \\mathbf{x}_i) < 1 \\text{) } \\right) + C \\times \\mathbf{w}$\n",
    "\n",
    "For the purposes of this tutorial, we initialize $\\mathbf{w}$ as an array of $\\mathbf{0's}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(w, X, y, C):  \n",
    "    hinge_loss = np.maximum(0, 1 - y * np.dot(X, w)) \n",
    "    regularization_term = 0.5 * np.dot(w, w)  # RegularizaciÃ³n L2 ###\n",
    "    total_loss = np.mean(hinge_loss) + C * regularization_term ###\n",
    "    return total_loss\n",
    "\n",
    "def loss_gradient(w, X, y, C):     \n",
    "    mask = (y * (np.dot(X, w))) < 1    #<1\n",
    "    gradient = (-np.dot(mask * y, X) / len(y)) +  C*w\n",
    "    return gradient\n",
    "\n",
    "# Gradiente descendente\n",
    "losses = []\n",
    "w = np.zeros(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Initial hyperparameters** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we declare the hyperparameters: learning rate (learning_rate), the number of epochs (num_epochs), and the regularization parameter (C). Then, we will use gradient descent to adjust the weights of the SVM model. For the purposes of this tutorial, we stick with the following hyperparameters; however, the hyperplane acquisition could be improved with their adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "C = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss = loss_function(w,X_train, y_train, C)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if epoch % 25 == 0 or epoch  == 99:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    gradient_w = loss_gradient(w, X_train, y_train,C)\n",
    "    w -= learning_rate * gradient_w\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model and observing the decrease of the loss function, we evaluate its performance on both the training and test data. We will calculate the accuracy and display the final loss on the training data. In our case, the weights $\\mathbf{w}$ and the accuracies will be the values against which we compare the SVM implementation in Cairo with Orion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Evaluate model on training data** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    return np.sign(np.dot(X, w))\n",
    "\n",
    "predictions = predict(X_train, w)\n",
    "final_loss = loss_function(w, X_train, y_train,C)\n",
    "\n",
    "print(\"Accuracy: {}\".format((predictions == y_train).mean()))\n",
    "print(\"Final loss: {}\".format(final_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Evaluate model on test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(X_test, w)\n",
    "\n",
    "print(\"Accuracy: {}\".format((predictions == y_test).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will visualize the obtained hyperplane, determined by $\\mathbf{w} = (-0.11463491, -0.35595999,  0.53122158,  0.24240275)$ and the way it separates the classes in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, cmap='autumn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Convert your model to Cairo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generating Cairo files**\n",
    "\n",
    "Now let's generate Cairo files for each tensor in the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_fp16x16(num):\n",
    "\n",
    "    whole_num = int(num)\n",
    "    fractional_part = int((num - whole_num) * 65536)\n",
    "    fp_number = (whole_num << 16) + fractional_part\n",
    "    return fp_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_name = [\"X_train\", \"Y_train\", \"X_test\", \"Y_test\", \"W\"]\n",
    "\n",
    "base_path = os.path.join(\"../../src\")\n",
    "\n",
    "def generate_cairo_files(data, name):\n",
    "    generated_path = os.path.join(base_path, 'generated')\n",
    "    os.makedirs(generated_path, exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(base_path, 'generated', f\"{name}.cairo\"), \"w\") as f:\n",
    "        f.write(\n",
    "            \"use array::ArrayTrait;\\n\" +\n",
    "            \"use orion::operators::tensor::{Tensor, TensorTrait, FP16x16Tensor};\\n\" +\n",
    "            \"use orion::numbers::{FixedTrait, FP16x16, FP16x16Impl};\\n\" +\n",
    "            \"\\n\" + f\"fn {name}() -> Tensor<FP16x16>\" + \"{\\n\\n\" + \n",
    "            \"let mut shape = ArrayTrait::new();\\n\"\n",
    "        )\n",
    "        for dim in data.shape:\n",
    "            f.write(f\"shape.append({dim});\\n\")\n",
    "    \n",
    "        f.write(\"let mut data = ArrayTrait::new();\")\n",
    "        for val in np.nditer(data.flatten()):\n",
    "            f.write(f\"data.append(FixedTrait::new({abs(int(decimal_to_fp16x16(val)))}, {str(val < 0).lower()}));\\n\")\n",
    "        f.write(\n",
    "            \"let tensor = TensorTrait::<FP16x16>::new(shape.span(), data.span());\\n\" +\n",
    "            \"return tensor;\\n}\"\n",
    "        )\n",
    "\n",
    "with open(os.path.join(base_path, 'generated.cairo'), 'w') as f:\n",
    "    for n in tensor_name:\n",
    "        f.write(f\"mod {n};\\n\")\n",
    "\n",
    "generate_cairo_files(X_train, \"X_train\")\n",
    "generate_cairo_files(X_test, \"X_test\")\n",
    "generate_cairo_files(y_train, \"Y_train\")\n",
    "generate_cairo_files(y_test, \"Y_test\")\n",
    "generate_cairo_files(w, \"W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Convert hyperparameters to FP16x16**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decimal_to_fp16x16(learning_rate))\n",
    "print(decimal_to_fp16x16(C))\n",
    "print(decimal_to_fp16x16(num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Get an estimate for the initial and final loss value, and final weights in FP16x16**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([decimal_to_fp16x16(w[0]),\n",
    "decimal_to_fp16x16(w[1]),\n",
    "decimal_to_fp16x16(w[2])])\n",
    "\n",
    "print(\"Initial loss: {}\".format(decimal_to_fp16x16(losses[0])))\n",
    "print(\"Final loss: {}\".format(decimal_to_fp16x16(final_loss)))\n",
    "print(\"Weights: {}\".format(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch ../../src/helper.cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../src/helper.cairo\n",
    "\n",
    "use debug::PrintTrait;\n",
    "use traits::TryInto;\n",
    "use array::{ArrayTrait, SpanTrait};\n",
    "use orion::operators::tensor::{\n",
    "    Tensor, TensorTrait, FP16x16Tensor, FP16x16TensorAdd, FP16x16TensorMul, FP16x16TensorSub,\n",
    "    FP16x16TensorDiv\n",
    "};\n",
    "use orion::numbers::{FixedTrait, FP16x16, FP16x16Impl};\n",
    "use orion::numbers::fixed_point::implementations::fp16x16::core::{\n",
    "    HALF, ONE, FP16x16Mul, FP16x16Div, FP16x16Print, FP16x16IntoI32, FP16x16PartialOrd,\n",
    "    FP16x16PartialEq\n",
    "};\n",
    "use giza::{\n",
    "    generated::{X_test::X_test, W::W}\n",
    "};\n",
    "\n",
    "\n",
    "// Returns an element-wise indication of the sign of a number.\n",
    "fn sign(z: @Tensor<FP16x16>) -> Tensor<FP16x16> {\n",
    "    let mut data_result = ArrayTrait::<FP16x16>::new();\n",
    "    let mut z_data = *z.data;\n",
    "\n",
    "    loop {\n",
    "        match z_data.pop_front() {\n",
    "            Option::Some(item) => {\n",
    "                let result = if *item.sign {\n",
    "                    FixedTrait::new(ONE, true)\n",
    "                } else {\n",
    "                    FixedTrait::new(ONE, false)\n",
    "                };\n",
    "                data_result.append(result);\n",
    "            },\n",
    "            Option::None(_) => {\n",
    "                break;\n",
    "            }\n",
    "        };\n",
    "    };\n",
    "\n",
    "    TensorTrait::<FP16x16>::new(*z.shape, data_result.span())\n",
    "}\n",
    "\n",
    "// Returns predictions using the machine learning model.\n",
    "fn main() -> Span<FP16x16> {\n",
    "    let x_test = X_test();\n",
    "    let final_w = W();\n",
    "    let result = sign(@(x_test.matmul(@final_w)));\n",
    "    x_test.data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! touch ../../src/lib.cairo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../../src/lib.cairo\n",
    "\n",
    "mod generated;\n",
    "mod helper;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run scarb build\n",
    "! scarb build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import platform\n",
    "\n",
    "def run_command_with_gnu_time(cmd, time_cmd):\n",
    "    full_cmd = f'{time_cmd} -f \"%M\" -- ' + ' '.join(cmd)\n",
    "    start_time = time.perf_counter()\n",
    "    result = subprocess.run(['bash', '-c', full_cmd], capture_output=True, text=True)\n",
    "    end_time = time.perf_counter()\n",
    "    print(\"Full Output:\")\n",
    "    print(result.stdout)\n",
    "    print(result.stderr)\n",
    "    # Get the total time\n",
    "    proving_time = end_time - start_time\n",
    "    # Get the proving time\n",
    "    if result.returncode != 0:\n",
    "        return (None, None)\n",
    "    return (result.stderr.strip(), proving_time)  # Memory usage is in stderr\n",
    "\n",
    "cmd = [\"scarb\", \"cairo-run\", \"--no-build\", \"--available-gas 99999999999999999\"]\n",
    "\n",
    "if platform.system() == 'Linux':\n",
    "    time_command = \"/usr/bin/time\"\n",
    "else:\n",
    "    time_command = \"gtime\"\n",
    "memory_usage, proving_time = run_command_with_gnu_time(cmd, time_command)\n",
    "\n",
    "# If memory usage is captured successfully\n",
    "if memory_usage:\n",
    "    memory_usage = f\"{memory_usage}kb\"\n",
    "else:\n",
    "    # throw an error\n",
    "    print(\"Error: Memory usage not captured\")\n",
    "    exit(1)\n",
    "\n",
    "# Rest of your process execution and timing code...\n",
    "    \n",
    "\n",
    "print(f\"Compilation Time: {proving_time} seconds\")\n",
    "print(f\"Memory Usage: {memory_usage}\")\n",
    "\n",
    "# define the path that stores the benchmarking results\n",
    "benchmark_path = os.path.join('../../benchmarks.json')\n",
    "\n",
    "with open(benchmark_path, 'r') as f:\n",
    "    benchmark = json.load(f)\n",
    "\n",
    "proving_time =str(proving_time) + \"s\"\n",
    "\n",
    "# Update the proving time in the loaded benchmark\n",
    "benchmark['svm_classifications']['orion']['provingTime'].append(proving_time)\n",
    "\n",
    "# Update the memory usage in the loaded benchmark\n",
    "benchmark['svm_classifications']['orion']['memoryUsage'].append(memory_usage)\n",
    "\n",
    "\n",
    "# Write the updated benchmark back to the file\n",
    "with open(benchmark_path, 'w') as f:\n",
    "    json.dump(benchmark, f, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cairo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
